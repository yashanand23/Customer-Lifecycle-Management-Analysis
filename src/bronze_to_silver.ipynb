{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92adbda0-52f1-43cf-b1e0-8028675f860d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import datetime\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Read Latest Bronze File\").getOrCreate()\n",
    "\n",
    "# Define Bronze Layer path\n",
    "bronze_path = \"dbfs:/FileStore/tables/bronze_layer1/\"\n",
    "\n",
    "# List all files in Bronze Layer\n",
    "files = dbutils.fs.ls(bronze_path)\n",
    "\n",
    "# Check if there are files\n",
    "if not files:\n",
    "    print(\"No files found in Bronze Layer.\")\n",
    "else:\n",
    "    # Extract filenames and modification timestamps\n",
    "    file_paths = [(file.path, file.modificationTime) for file in files]\n",
    "\n",
    "    # Sort files based on modification time (latest first)\n",
    "    file_paths.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get latest file path\n",
    "    latest_file_path = file_paths[0][0]\n",
    "    print(f\"Latest file in Bronze Layer: {latest_file_path}\")\n",
    "\n",
    "    # Read the latest file into a DataFrame\n",
    "    df_bronze = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"quote\", '\"') \\\n",
    "        .option(\"escape\", '\"') \\\n",
    "        .option(\"multiLine\", \"true\") \\\n",
    "        .load(latest_file_path)\n",
    "\n",
    "    # Show DataFrame\n",
    "    df_bronze.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e0e5a3e-3c8e-4ed2-9cc1-440751d5e551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86fa3879-7ebd-41f4-aab3-cd479bc2e1aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de447d49-d0b4-47f9-aad2-afccbff0b87d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, explode, from_json\n",
    "from pyspark.sql.types import StructType, ArrayType\n",
    "from typing import List, Tuple\n",
    "\n",
    "def flatten_df_generic(df_bronze: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Dynamically flatten a DataFrame while properly handling nested JSON and arrays.\n",
    "\n",
    "    Args:\n",
    "        df_bronze: Input DataFrame (Raw Data from the Bronze Layer).\n",
    "\n",
    "    Returns:\n",
    "        Flattened DataFrame with all nested structures expanded into columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def process_json_columns(df: DataFrame) -> Tuple[DataFrame, List[str]]:\n",
    "        \"\"\"Convert JSON string columns into structured columns and track column order.\"\"\"\n",
    "        json_columns = []\n",
    "        for column in df.columns:\n",
    "            sample = df.select(column).first()[0]\n",
    "            if sample is not None:\n",
    "                try:\n",
    "                    if isinstance(sample, str):\n",
    "                        if sample[0] == '{':\n",
    "                            # Convert JSON object string to a structured column\n",
    "                            schema = spark.read.json(df.rdd.map(lambda x: x[column])).schema\n",
    "                            df = df.withColumn(column, from_json(col(column), schema))\n",
    "                            json_columns.append(column)\n",
    "                        elif sample[0] == '[':\n",
    "                            json_columns.append(column)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        return df, json_columns\n",
    "\n",
    "    def explode_arrays(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Explode array columns into multiple rows.\"\"\"\n",
    "        for column in df.columns:\n",
    "            if isinstance(df.schema[column].dataType, ArrayType):\n",
    "                df = df.withColumn(column, explode(col(column)))\n",
    "        return df\n",
    "\n",
    "    def flatten_structs(df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Flatten struct columns into individual fields and track column order.\"\"\"\n",
    "        new_columns = []\n",
    "        for field in df.schema.fields:\n",
    "            if isinstance(field.dataType, StructType):\n",
    "                field_name = field.name\n",
    "                for nested_field in field.dataType.fields:\n",
    "                    nested_name = f\"{field_name}_{nested_field.name}\"\n",
    "                    df = df.withColumn(nested_name, col(f\"{field_name}.{nested_field.name}\"))\n",
    "                    new_columns.append(nested_name)\n",
    "                df = df.drop(field_name)\n",
    "            else:\n",
    "                new_columns.append(field.name)\n",
    "        return df, new_columns\n",
    "\n",
    "    def extract_json_paths(df: DataFrame, json_columns: List[str]) -> DataFrame:\n",
    "        \"\"\"Extract fields from JSON string columns.\"\"\"\n",
    "        for column in json_columns:\n",
    "            sample = df.select(column).first()[0]\n",
    "            if sample and sample[0] == '[':\n",
    "                array_schema = spark.read.json(df.rdd.map(lambda x: x[column])).schema\n",
    "                df = df.withColumn(column, from_json(col(column), ArrayType(array_schema)))\n",
    "            elif sample and sample[0] == '{':\n",
    "                object_schema = spark.read.json(df.rdd.map(lambda x: x[column])).schema\n",
    "                df = df.withColumn(column, from_json(col(column), object_schema))\n",
    "        return df\n",
    "\n",
    "    # Step 1: Process JSON columns and track column order\n",
    "    df_bronze, json_columns = process_json_columns(df_bronze)\n",
    "\n",
    "    # Step 2: Extract JSON paths for identified columns\n",
    "    df_bronze = extract_json_paths(df_bronze, json_columns)\n",
    "\n",
    "    # Step 3: Explode arrays into individual rows\n",
    "    df_bronze = explode_arrays(df_bronze)\n",
    "\n",
    "    # Step 4: Flatten all structs and track column order\n",
    "    df_bronze, final_columns = flatten_structs(df_bronze)\n",
    "\n",
    "    # Repeat Steps 1-4 until no more nested columns\n",
    "    nested_columns = [field.name for field in df_bronze.schema.fields if isinstance(field.dataType, (StructType, ArrayType))]\n",
    "    while nested_columns:\n",
    "        df_bronze = explode_arrays(flatten_structs(df_bronze)[0])\n",
    "        nested_columns = [field.name for field in df_bronze.schema.fields if isinstance(field.dataType, (StructType, ArrayType))]\n",
    "\n",
    "    # Select columns in the order they were processed\n",
    "    return df_bronze.select([col(c) for c in final_columns])\n",
    "\n",
    "# Process the raw data from Bronze Layer\n",
    "flattened_df = flatten_df_generic(df_bronze)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b846180-f2ea-428e-8dd9-fe89d4e695c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Flatten the input DataFrame\n",
    "flattened_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5b6bbec-ac6f-4da1-acd7-bf3b45ae33a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flattened_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c4d78f-be88-4cdb-bdf6-18bf4adee587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "class DataCleaner:\n",
    "    def __init__(self, flattened_df):\n",
    "        self.flattened_df = flattened_df\n",
    "        # Automatically categorize columns by data type\n",
    "        self.schema = {f.name: f.dataType for f in flattened_df.schema.fields}\n",
    "        self.numeric_cols = [f.name for f in flattened_df.schema.fields if isinstance(f.dataType, (DoubleType, LongType, IntegerType))]\n",
    "        self.string_cols = [f.name for f in flattened_df.schema.fields if isinstance(f.dataType, StringType)]\n",
    "        self.date_cols = [col for col in self.string_cols if 'date' in col.lower() or 'time' in col.lower()]\n",
    "\n",
    "    def clean_numeric_columns(self):\n",
    "        \"\"\"Clean numeric columns - handle nulls, outliers, and retain original data types.\"\"\"\n",
    "        for col_name in self.numeric_cols:\n",
    "            # Get original data type\n",
    "            original_type = self.schema[col_name]\n",
    "\n",
    "            # Calculate statistics for outlier detection\n",
    "            stats = self.flattened_df.select(\n",
    "                mean(col(col_name)).alias('mean'),\n",
    "                stddev(col(col_name)).alias('stddev'),\n",
    "                percentile_approx(col(col_name), 0.25).alias('q1'),\n",
    "                percentile_approx(col(col_name), 0.75).alias('q3')\n",
    "            ).collect()[0]\n",
    "\n",
    "            mean_val = stats['mean']\n",
    "            q1 = stats['q1']\n",
    "            q3 = stats['q3']\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "            # Apply transformations\n",
    "            cleaned_col = when(col(col_name).isNull(), mean_val) \\\n",
    "                .when(col(col_name) < lower_bound, lower_bound) \\\n",
    "                .when(col(col_name) > upper_bound, upper_bound) \\\n",
    "                .otherwise(col(col_name))\n",
    "\n",
    "            # Cast the cleaned column back to its original type\n",
    "            if isinstance(original_type, IntegerType):\n",
    "                cleaned_col = cleaned_col.cast(IntegerType())\n",
    "            elif isinstance(original_type, LongType):\n",
    "                cleaned_col = cleaned_col.cast(LongType())\n",
    "            elif isinstance(original_type, DoubleType):\n",
    "                cleaned_col = cleaned_col.cast(DoubleType())\n",
    "\n",
    "            # Replace the column with the cleaned version\n",
    "            self.flattened_df = self.flattened_df.withColumn(col_name, cleaned_col)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def clean_string_columns(self):\n",
    "        \"\"\"Clean string columns - handle nulls, standardize case, remove special characters\"\"\"\n",
    "        for col_name in self.string_cols:\n",
    "            if col_name not in self.date_cols:  # Skip date/time columns\n",
    "                self.flattened_df = self.flattened_df.withColumn(\n",
    "                    col_name,\n",
    "                    when(col(col_name).isNull() | (trim(col(col_name)) == \"\"), \"unknown\")\n",
    "                    .otherwise(\n",
    "                        regexp_replace(  # Remove special characters except space and hyphen\n",
    "                            trim(lower(col(col_name))),  # Standardize case and remove whitespace\n",
    "                            '[^a-z0-9\\\\s-]', ''\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "        return self\n",
    "\n",
    "    def clean_date_time_columns(self):\n",
    "        \"\"\"Clean and standardize date and time columns, ensuring action_ts is properly formatted\"\"\"\n",
    "        for col_name in self.date_cols:\n",
    "            if 'date' in col_name.lower():\n",
    "                # Convert to standard date format YYYY-MM-DD\n",
    "                self.flattened_df = self.flattened_df.withColumn(\n",
    "                    col_name,\n",
    "                    when(to_date(col(col_name), 'yyyy-MM-dd').isNull(), current_date())\n",
    "                    .otherwise(to_date(col(col_name), 'yyyy-MM-dd'))\n",
    "                )\n",
    "            elif 'time' in col_name.lower() and col_name != \"action_ts\":\n",
    "                # Convert to standard time format HH:mm:ss\n",
    "                self.flattened_df = self.flattened_df.withColumn(\n",
    "                    col_name,\n",
    "                    when(\n",
    "                        regexp_extract(col(col_name), '^(?:[01]\\\\d|2[0-3]):[0-5]\\\\d:[0-5]\\\\d$', 0) == '',\n",
    "                        lit('00:00:00')\n",
    "                    ).otherwise(col(col_name))\n",
    "                )\n",
    "            elif col_name == \"action_ts\":\n",
    "                # Ensure action_ts is properly formatted as a timestamp\n",
    "                self.flattened_df = self.flattened_df.withColumn(\n",
    "                    col_name,\n",
    "                    when(to_timestamp(col(col_name), 'yyyy-MM-dd HH:mm:ss').isNull(), lit(\"1970-01-01 00:00:00\"))\n",
    "                    .otherwise(to_timestamp(col(col_name), 'yyyy-MM-dd HH:mm:ss'))\n",
    "                )\n",
    "        return self\n",
    "\n",
    "    def standardize_categorical_columns(self):\n",
    "        \"\"\"Standardize categorical columns generically\"\"\"\n",
    "        for col_name in self.string_cols:\n",
    "            if col_name not in self.date_cols:\n",
    "                self.flattened_df = self.flattened_df.withColumn(\n",
    "                    col_name,\n",
    "                    lower(trim(col(col_name)))  # Convert to lowercase and trim spaces\n",
    "                )\n",
    "        return self\n",
    "\n",
    "    def handle_duplicates(self, subset=None):\n",
    "        \"\"\"Remove duplicate records based on a subset of columns or the entire dataframe\"\"\"\n",
    "        if subset:\n",
    "            self.flattened_df = self.flattened_df.dropDuplicates(subset)\n",
    "        else:\n",
    "            self.flattened_df = self.flattened_df.dropDuplicates()\n",
    "        return self    \n",
    "\n",
    "    def apply_all_transformations(self):\n",
    "        \"\"\"Apply all cleaning and transformation steps\"\"\"\n",
    "        return (self\n",
    "                .clean_numeric_columns()\n",
    "                .clean_string_columns()\n",
    "                .clean_date_time_columns()\n",
    "                .standardize_categorical_columns()\n",
    "                .handle_duplicates()\n",
    "                .flattened_df)\n",
    "\n",
    "# Usage Example\n",
    "def process_dataframe(flattened_df):\n",
    "    try:\n",
    "        # Initialize cleaner\n",
    "        cleaner = DataCleaner(flattened_df)\n",
    "        \n",
    "        # Apply all transformations\n",
    "        cleaned_df = cleaner.apply_all_transformations()\n",
    "        \n",
    "        # Cache the cleaned dataframe\n",
    "        cleaned_df.cache()\n",
    "        \n",
    "        # Return success status and cleaned dataframe\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"data\": cleaned_df,\n",
    "            \"row_count\": cleaned_df.count()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error_message\": str(e)\n",
    "        }\n",
    "\n",
    "# Process the DataFrame and display results\n",
    "result = process_dataframe(flattened_df)\n",
    "\n",
    "if result[\"status\"] == \"success\":\n",
    "    cleaned_df = result[\"data\"]\n",
    "    cleaned_df.display()\n",
    "else:\n",
    "    print(f\"Error: {result['error_message']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c78a68d-27f7-419e-9a69-436ea955e424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec7bd57-f610-44d6-89ac-952409e142cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Spark session with Delta Lake support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL Pipeline\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0,com.amazon.redshift:redshift-jdbc42:2.1.0.18\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Redshift connection details\n",
    "redshift_url = \"jdbc:redshift://clusterid.end-point.aws-region.redshift.amazonaws.com:port-number/redshift-database-name\"\n",
    "redshift_properties = {\n",
    "    \"user\": \"redshift_user_name\",\n",
    "    \"password\": \"redshift_password\",\n",
    "    \"driver\": \"com.amazon.redshift.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "table_name = \"bronze_to_stagging\"\n",
    "etl_tracker_table = \"gold.etl_tracker\"\n",
    "\n",
    "# Define S3 paths\n",
    "aws_bucket_name = \"clm-case-study\"\n",
    "s3_folder = \"silver-layer\"\n",
    "s3_output_path = f\"s3a://{aws_bucket_name}/{s3_folder}/\"\n",
    "\n",
    "# Set AWS credentials\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"access-key\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", \"access-secret-key\")\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "def get_latest_last_modified():\n",
    "    \"\"\"Fetch the latest processed timestamp from Redshift ETL tracker.\"\"\"\n",
    "    query = f\"\"\"\n",
    "        (SELECT MAX(timestamp) AS last_modified_at \n",
    "         FROM {etl_tracker_table} WHERE table_name = '{table_name}') AS latest_timestamp\"\"\"\n",
    "    etl_tracker_df = spark.read.jdbc(url=redshift_url, table=query, properties=redshift_properties)\n",
    "    \n",
    "    latest_timestamp = etl_tracker_df.collect()[0][0]\n",
    "    return latest_timestamp if latest_timestamp else \"1900-01-01 00:00:00\"\n",
    "\n",
    "def update_etl_tracker(new_max_timestamp):\n",
    "    \"\"\"Update the ETL tracker in Redshift using Spark JDBC.\"\"\"\n",
    "    update_data = [(table_name, new_max_timestamp, \"silver\", \"success\")]\n",
    "    update_df = spark.createDataFrame(update_data, [\"table_name\", \"timestamp\", \"layer\", \"status\"])\n",
    "    \n",
    "    update_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", redshift_url) \\\n",
    "        .option(\"dbtable\", etl_tracker_table) \\\n",
    "        .options(**redshift_properties) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "try:\n",
    "    # Get latest processed timestamp from ETL tracker\n",
    "    latest_timestamp = get_latest_last_modified()\n",
    "    logger.info(f\"Latest processed timestamp from ETL tracker: {latest_timestamp}\")\n",
    "    \n",
    "    # Filter new data based on last_modified_at\n",
    "    new_data_df = cleaned_df.filter(col(\"last_modified_at\") > latest_timestamp).coalesce(1)\n",
    "    new_records_count = new_data_df.count()\n",
    "    \n",
    "    if new_records_count == 0:\n",
    "        logger.info(\"No new data to load\")\n",
    "    else:\n",
    "        logger.info(f\"Loading {new_records_count} new records\")\n",
    "        \n",
    "        # Append new data to Delta Lake with coalesce\n",
    "        new_data_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save(s3_output_path)\n",
    "        \n",
    "        # Update ETL tracker with new max timestamp\n",
    "        new_max_timestamp = new_data_df.agg(max(\"last_modified_at\")).collect()[0][0]\n",
    "        update_etl_tracker(new_max_timestamp)\n",
    "        logger.info(f\"Updated ETL tracker with new max timestamp: {new_max_timestamp}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during ETL process: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7318e755-0e34-488c-987c-8414e1cc681f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_to_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
