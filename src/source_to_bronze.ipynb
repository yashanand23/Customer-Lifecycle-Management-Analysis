{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2db5760-242d-4ebe-ba5b-0699b16b8b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Latest ETL Timestamp from Redshift: None\n",
      "üöÄ Processing 1 new/modified files for Bronze Layer.\n",
      "‚úÖ Current ETL Timestamp: 2025-02-27 07:36:39.256743\n",
      "‚úÖ ETL Tracker updated in Redshift with timestamp: 2025-02-27 07:36:39.256743\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max as sql_max, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "import datetime\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Raw to Bronze ETL\").getOrCreate()\n",
    "\n",
    "# Redshift JDBC Connection Details\n",
    "redshift_jdbc_url = \"jdbc:redshift://redshift-cluster-id.end-point.aws-region.redshift.amazonaws.com:port-number/redshift-database-name\"\n",
    "redshift_properties = {\n",
    "    \"user\": \"redshift_user_name\",\n",
    "    \"password\": \"redshift-password\",\n",
    "    \"driver\": \"com.amazon.redshift.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "table_name = \"raw_to_bronze\"\n",
    "\n",
    "# Fetch latest ETL timestamp from Redshift\n",
    "def get_latest_etl_timestamp():\n",
    "    try:\n",
    "        query = f\"(SELECT MAX(timestamp) AS last_run FROM gold.etl_tracker WHERE table_name = '{table_name}') AS last_etl\"\n",
    "        etl_tracker_df = spark.read.jdbc(url=redshift_jdbc_url, table=query, properties=redshift_properties)\n",
    "        \n",
    "        # Extract latest timestamp\n",
    "        if etl_tracker_df.count() > 0 and etl_tracker_df.collect()[0][\"last_run\"] is not None:\n",
    "            return etl_tracker_df.collect()[0][\"last_run\"]\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching latest ETL timestamp from Redshift: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "latest_etl_timestamp = get_latest_etl_timestamp()\n",
    "print(f\"‚úÖ Latest ETL Timestamp from Redshift: {latest_etl_timestamp}\")\n",
    "\n",
    "# Define paths\n",
    "source_path = \"dbfs:/FileStore/tables/raw_data1/\"\n",
    "bronze_path = \"dbfs:/FileStore/tables/bronze_layer1/\"\n",
    "\n",
    "# List files in Raw Layer\n",
    "raw_files = dbutils.fs.ls(source_path)\n",
    "\n",
    "# Filter files based on modification time (for incremental load)\n",
    "if latest_etl_timestamp:\n",
    "    new_files = [f for f in raw_files if datetime.datetime.fromtimestamp(f.modificationTime / 1000.0) > latest_etl_timestamp]\n",
    "else:\n",
    "    new_files = raw_files  # Full load if no previous ETL run found\n",
    "\n",
    "if new_files:\n",
    "    print(f\"üöÄ Processing {len(new_files)} new/modified files for Bronze Layer.\")\n",
    "\n",
    "    # Copy new files from Raw to Bronze and keep track of their paths\n",
    "    copied_files = []\n",
    "    \n",
    "    for file in new_files:\n",
    "        target_path = f\"{bronze_path}{file.name}\"\n",
    "        dbutils.fs.cp(file.path, target_path)\n",
    "        copied_files.append(target_path)\n",
    "\n",
    "    # Get the current timestamp for ETL tracking\n",
    "    current_etl_timestamp = datetime.datetime.utcnow()\n",
    "    print(f\"‚úÖ Current ETL Timestamp: {current_etl_timestamp}\")\n",
    "\n",
    "    # Update ETL Tracker in Redshift\n",
    "    try:\n",
    "        df_update = spark.createDataFrame(\n",
    "            [(table_name, current_etl_timestamp, 'bronze', 'success')],\n",
    "            [\"table_name\", \"timestamp\", \"layer\", \"status\"]\n",
    "        )\n",
    "\n",
    "        # Write to Redshift\n",
    "        df_update.write.jdbc(\n",
    "            url=redshift_jdbc_url,\n",
    "            table=\"gold.etl_tracker\",\n",
    "            mode=\"append\",\n",
    "            properties=redshift_properties\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ ETL Tracker updated in Redshift with timestamp: {current_etl_timestamp}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error updating ETL tracker in Redshift: {str(e)}\")\n",
    "else:\n",
    "    print(\"‚úÖ No new files found. Nothing to copy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c4a4f93-54a4-4566-b316-51b2c8e2313b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Source_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
